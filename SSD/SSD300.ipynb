{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from math import sqrt\n",
    "import numpy as np\n",
    "from IPython.display import clear_output\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "import time\n",
    "\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "sns.color_palette(\"bright\")\n",
    "import matplotlib as mpl\n",
    "import matplotlib.cm as cm\n",
    "\n",
    "import torch\n",
    "from torch import Tensor\n",
    "from torch import nn\n",
    "from torch.nn  import functional as F \n",
    "from torch.autograd import Variable\n",
    "from itertools import product as product\n",
    "import torchvision\n",
    "import torch.backends.cudnn as cudnn\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "from PIL import Image\n",
    "import json\n",
    "import os\n",
    "import random\n",
    "import xml.etree.ElementTree as ET\n",
    "import torchvision.transforms.functional as FT\n",
    "import cv2\n",
    "from pprint import PrettyPrinter\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decimate(tensor, m):\n",
    "    \"\"\"\n",
    "    Decimate a tensor by a factor 'm', i.e. downsample by keeping every 'm'th value.\n",
    "    This is used when we convert FC layers to equivalent Convolutional layers, BUT of a smaller size.\n",
    "    :param tensor: tensor to be decimated\n",
    "    :param m: list of decimation factors for each dimension of the tensor; None if not to be decimated along a dimension\n",
    "    :return: decimated tensor\n",
    "    \"\"\"\n",
    "    assert tensor.dim() == len(m)\n",
    "    for d in range(tensor.dim()):\n",
    "        if m[d] is not None:\n",
    "            tensor = tensor.index_select(dim=d,\n",
    "                                         index=torch.arange(start=0, end=tensor.size(d), step=m[d]).long())\n",
    "\n",
    "    return tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VGGBase(nn.Module):\n",
    "    \"\"\"\n",
    "    base network: VGG16\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        super(VGGBase, self).__init__()\n",
    "\n",
    "        # VGG16网络的卷积和池化\n",
    "        self.conv1_1 = nn.Conv2d(3, 64, kernel_size=3, padding=1)  # stride = 1, by default\n",
    "        self.conv1_2 = nn.Conv2d(64, 64, kernel_size=3, padding=1)\n",
    "        self.pool1 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "\n",
    "        self.conv2_1 = nn.Conv2d(64, 128, kernel_size=3, padding=1)\n",
    "        self.conv2_2 = nn.Conv2d(128, 128, kernel_size=3, padding=1)\n",
    "        self.pool2 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "\n",
    "        self.conv3_1 = nn.Conv2d(128, 256, kernel_size=3, padding=1)\n",
    "        self.conv3_2 = nn.Conv2d(256, 256, kernel_size=3, padding=1)\n",
    "        self.conv3_3 = nn.Conv2d(256, 256, kernel_size=3, padding=1)\n",
    "        self.pool3 = nn.MaxPool2d(kernel_size=2, stride=2, ceil_mode=True)  # ceiling (not floor) here for even dims\n",
    "\n",
    "        self.conv4_1 = nn.Conv2d(256, 512, kernel_size=3, padding=1)\n",
    "        self.conv4_2 = nn.Conv2d(512, 512, kernel_size=3, padding=1)\n",
    "        self.conv4_3 = nn.Conv2d(512, 512, kernel_size=3, padding=1)\n",
    "        self.pool4 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "\n",
    "        self.conv5_1 = nn.Conv2d(512, 512, kernel_size=3, padding=1)\n",
    "        self.conv5_2 = nn.Conv2d(512, 512, kernel_size=3, padding=1)\n",
    "        self.conv5_3 = nn.Conv2d(512, 512, kernel_size=3, padding=1)\n",
    "        self.pool5 = nn.MaxPool2d(kernel_size=3, stride=1, padding=1)  # retains size because stride is 1 (and padding)\n",
    "\n",
    "        # fc6和fc7成为转化为卷积层\n",
    "        self.conv6 = nn.Conv2d(512, 1024, kernel_size=3, padding=6, dilation=6)  # atrous convolution\n",
    "\n",
    "        self.conv7 = nn.Conv2d(1024, 1024, kernel_size=1)\n",
    "\n",
    "        # Load pretrained layers\n",
    "        self.load_pretrained_layers()\n",
    "\n",
    "    def forward(self, image):\n",
    "        \"\"\"\n",
    "        VGG16网络的前向传播\n",
    "        输入为image: 图像tensor, 维度 (N, 3, 300, 300)\n",
    "        输出是conv4_3和conv7的feature map\n",
    "        \"\"\"\n",
    "        out = F.relu(self.conv1_1(image))  # (N, 64, 300, 300)\n",
    "        out = F.relu(self.conv1_2(out))  # (N, 64, 300, 300)\n",
    "        out = self.pool1(out)  # (N, 64, 150, 150)\n",
    "\n",
    "        out = F.relu(self.conv2_1(out))  # (N, 128, 150, 150)\n",
    "        out = F.relu(self.conv2_2(out))  # (N, 128, 150, 150)\n",
    "        out = self.pool2(out)  # (N, 128, 75, 75)\n",
    "\n",
    "        out = F.relu(self.conv3_1(out))  # (N, 256, 75, 75)\n",
    "        out = F.relu(self.conv3_2(out))  # (N, 256, 75, 75)\n",
    "        out = F.relu(self.conv3_3(out))  # (N, 256, 75, 75)\n",
    "        out = self.pool3(out)  # (N, 256, 38, 38), it would have been 37 if not for ceil_mode = True\n",
    "\n",
    "        out = F.relu(self.conv4_1(out))  # (N, 512, 38, 38)\n",
    "        out = F.relu(self.conv4_2(out))  # (N, 512, 38, 38)\n",
    "        out = F.relu(self.conv4_3(out))  # (N, 512, 38, 38)\n",
    "        conv4_3_feats = out  # (N, 512, 38, 38)\n",
    "        out = self.pool4(out)  # (N, 512, 19, 19)\n",
    "\n",
    "        out = F.relu(self.conv5_1(out))  # (N, 512, 19, 19)\n",
    "        out = F.relu(self.conv5_2(out))  # (N, 512, 19, 19)\n",
    "        out = F.relu(self.conv5_3(out))  # (N, 512, 19, 19)\n",
    "        out = self.pool5(out)  # (N, 512, 19, 19), pool5 does not reduce dimensions\n",
    "\n",
    "        out = F.relu(self.conv6(out))  # (N, 1024, 19, 19)\n",
    "\n",
    "        conv7_feats = F.relu(self.conv7(out))  # (N, 1024, 19, 19)\n",
    "\n",
    "        # Lower-level feature maps\n",
    "        return conv4_3_feats, conv7_feats\n",
    "\n",
    "    def load_pretrained_layers(self):\n",
    "        \"\"\"\n",
    "        加载预训练的网络，第六层和第七层要做相应的转化\n",
    "        \"\"\"\n",
    "        # Current state of base\n",
    "        state_dict = self.state_dict()\n",
    "        param_names = list(state_dict.keys())\n",
    "\n",
    "        # Pretrained VGG base\n",
    "        pretrained_state_dict = torchvision.models.vgg16(pretrained=True).state_dict()\n",
    "        pretrained_param_names = list(pretrained_state_dict.keys())\n",
    "\n",
    "        # Transfer conv. parameters from pretrained model to current model\n",
    "        for i, param in enumerate(param_names[:-4]):  # excluding conv6 and conv7 parameters\n",
    "            state_dict[param] = pretrained_state_dict[pretrained_param_names[i]]\n",
    "\n",
    "        # Convert fc6, fc7 to convolutional layers, and subsample (by decimation) to sizes of conv6 and conv7\n",
    "        # fc6\n",
    "        conv_fc6_weight = pretrained_state_dict['classifier.0.weight'].view(4096, 512, 7, 7)  # (4096, 512, 7, 7)\n",
    "        conv_fc6_bias = pretrained_state_dict['classifier.0.bias']  # (4096)\n",
    "        state_dict['conv6.weight'] = decimate(conv_fc6_weight, m=[4, None, 3, 3])  # (1024, 512, 3, 3)\n",
    "        state_dict['conv6.bias'] = decimate(conv_fc6_bias, m=[4])  # (1024)\n",
    "        # fc7\n",
    "        conv_fc7_weight = pretrained_state_dict['classifier.3.weight'].view(4096, 4096, 1, 1)  # (4096, 4096, 1, 1)\n",
    "        conv_fc7_bias = pretrained_state_dict['classifier.3.bias']  # (4096)\n",
    "        state_dict['conv7.weight'] = decimate(conv_fc7_weight, m=[4, 4, None, None])  # (1024, 1024, 1, 1)\n",
    "        state_dict['conv7.bias'] = decimate(conv_fc7_bias, m=[4])  # (1024)\n",
    "\n",
    "        self.load_state_dict(state_dict)\n",
    "\n",
    "        print(\"\\nLoaded base model.\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AuxiliaryConvolutions(nn.Module):\n",
    "    \"\"\"\n",
    "    用来提取特征的辅助卷积层\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        super(AuxiliaryConvolutions, self).__init__()\n",
    "\n",
    "        \n",
    "        self.conv8_1 = nn.Conv2d(1024, 256, kernel_size=1, padding=0)  # stride = 1, by default\n",
    "        self.conv8_2 = nn.Conv2d(256, 512, kernel_size=3, stride=2, padding=1)  # dim. reduction because stride > 1\n",
    "\n",
    "        self.conv9_1 = nn.Conv2d(512, 128, kernel_size=1, padding=0)\n",
    "        self.conv9_2 = nn.Conv2d(128, 256, kernel_size=3, stride=2, padding=1)  # dim. reduction because stride > 1\n",
    "\n",
    "        self.conv10_1 = nn.Conv2d(256, 128, kernel_size=1, padding=0)\n",
    "        self.conv10_2 = nn.Conv2d(128, 256, kernel_size=3, padding=0)  # dim. reduction because padding = 0\n",
    "\n",
    "        self.conv11_1 = nn.Conv2d(256, 128, kernel_size=1, padding=0)\n",
    "        self.conv11_2 = nn.Conv2d(128, 256, kernel_size=3, padding=0)  # dim. reduction because padding = 0\n",
    "\n",
    "        # 初始化\n",
    "        self.init_conv2d()\n",
    "\n",
    "    def init_conv2d(self):\n",
    "        \"\"\"\n",
    "        初始化\n",
    "        \"\"\"\n",
    "        for c in self.children():\n",
    "            if isinstance(c, nn.Conv2d):\n",
    "                nn.init.xavier_uniform_(c.weight)       # xavier初始化\n",
    "                nn.init.constant_(c.bias, 0.)\n",
    "\n",
    "    def forward(self, conv7_feats):\n",
    "        \"\"\"\n",
    "        输入conv7_feats: conv7的feature map, 维度(N, 1024, 19, 19)\n",
    "        输出: feature maps conv8_2, conv9_2, conv10_2和conv11_2\n",
    "        \"\"\"\n",
    "        out = F.relu(self.conv8_1(conv7_feats))  # (N, 256, 19, 19)\n",
    "        out = F.relu(self.conv8_2(out))  # (N, 512, 10, 10)\n",
    "        conv8_2_feats = out  # (N, 512, 10, 10)\n",
    "\n",
    "        out = F.relu(self.conv9_1(out))  # (N, 128, 10, 10)\n",
    "        out = F.relu(self.conv9_2(out))  # (N, 256, 5, 5)\n",
    "        conv9_2_feats = out  # (N, 256, 5, 5)\n",
    "\n",
    "        out = F.relu(self.conv10_1(out))  # (N, 128, 5, 5)\n",
    "        out = F.relu(self.conv10_2(out))  # (N, 256, 3, 3)\n",
    "        conv10_2_feats = out  # (N, 256, 3, 3)\n",
    "\n",
    "        out = F.relu(self.conv11_1(out))  # (N, 128, 3, 3)\n",
    "        conv11_2_feats = F.relu(self.conv11_2(out))  # (N, 256, 1, 1)\n",
    "\n",
    "        # Higher-level feature maps\n",
    "        return conv8_2_feats, conv9_2_feats, conv10_2_feats, conv11_2_feats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PredictionConvolutions(nn.Module):\n",
    "    \"\"\"\n",
    "    用前两类卷积输出的feature map进行位置和分类的预测, 预测值对应了8732个prior的特征\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, n_classes):\n",
    "        \"\"\"\n",
    "        :param n_classes: number of different types of objects\n",
    "        \"\"\"\n",
    "        super(PredictionConvolutions, self).__init__()\n",
    "\n",
    "        self.n_classes = n_classes\n",
    "\n",
    "        # feature map上每个像素的box数量\n",
    "        n_boxes = {'conv4_3': 4,\n",
    "                   'conv7': 6,\n",
    "                   'conv8_2': 6,\n",
    "                   'conv9_2': 6,\n",
    "                   'conv10_2': 4,\n",
    "                   'conv11_2': 4}\n",
    "        \n",
    "        # 用于位置预测的卷积层\n",
    "        self.loc_conv4_3 = nn.Conv2d(512, n_boxes['conv4_3'] * 4, kernel_size=3, padding=1)\n",
    "        self.loc_conv7 = nn.Conv2d(1024, n_boxes['conv7'] * 4, kernel_size=3, padding=1)\n",
    "        self.loc_conv8_2 = nn.Conv2d(512, n_boxes['conv8_2'] * 4, kernel_size=3, padding=1)\n",
    "        self.loc_conv9_2 = nn.Conv2d(256, n_boxes['conv9_2'] * 4, kernel_size=3, padding=1)\n",
    "        self.loc_conv10_2 = nn.Conv2d(256, n_boxes['conv10_2'] * 4, kernel_size=3, padding=1)\n",
    "        self.loc_conv11_2 = nn.Conv2d(256, n_boxes['conv11_2'] * 4, kernel_size=3, padding=1)\n",
    "\n",
    "        # 用于分类预测的卷积层\n",
    "        self.cl_conv4_3 = nn.Conv2d(512, n_boxes['conv4_3'] * n_classes, kernel_size=3, padding=1)\n",
    "        self.cl_conv7 = nn.Conv2d(1024, n_boxes['conv7'] * n_classes, kernel_size=3, padding=1)\n",
    "        self.cl_conv8_2 = nn.Conv2d(512, n_boxes['conv8_2'] * n_classes, kernel_size=3, padding=1)\n",
    "        self.cl_conv9_2 = nn.Conv2d(256, n_boxes['conv9_2'] * n_classes, kernel_size=3, padding=1)\n",
    "        self.cl_conv10_2 = nn.Conv2d(256, n_boxes['conv10_2'] * n_classes, kernel_size=3, padding=1)\n",
    "        self.cl_conv11_2 = nn.Conv2d(256, n_boxes['conv11_2'] * n_classes, kernel_size=3, padding=1)\n",
    "\n",
    "        # 初始化\n",
    "        self.init_conv2d()\n",
    "\n",
    "    def init_conv2d(self):\n",
    "        \"\"\"\n",
    "        初始化\n",
    "        \"\"\"\n",
    "        for c in self.children():\n",
    "            if isinstance(c, nn.Conv2d):\n",
    "                nn.init.xavier_uniform_(c.weight)\n",
    "                nn.init.constant_(c.bias, 0.)\n",
    "\n",
    "    def forward(self, conv4_3_feats, conv7_feats, conv8_2_feats, conv9_2_feats, conv10_2_feats, conv11_2_feats):\n",
    "        \"\"\"\n",
    "        前向传播的输入\n",
    "        :param conv4_3_feats: conv4_3 feature map, a tensor of dimensions (N, 512, 38, 38)\n",
    "        :param conv7_feats: conv7 feature map, a tensor of dimensions (N, 1024, 19, 19)\n",
    "        :param conv8_2_feats: conv8_2 feature map, a tensor of dimensions (N, 512, 10, 10)\n",
    "        :param conv9_2_feats: conv9_2 feature map, a tensor of dimensions (N, 256, 5, 5)\n",
    "        :param conv10_2_feats: conv10_2 feature map, a tensor of dimensions (N, 256, 3, 3)\n",
    "        :param conv11_2_feats: conv11_2 feature map, a tensor of dimensions (N, 256, 1, 1)\n",
    "        :return: 8732 locations and class scores (i.e. w.r.t each prior box) for each image\n",
    "        \"\"\"\n",
    "        batch_size = conv4_3_feats.size(0)\n",
    "\n",
    "        # 位置预测的输出\n",
    "        l_conv4_3 = self.loc_conv4_3(conv4_3_feats)  # (N, 16, 38, 38)\n",
    "        l_conv4_3 = l_conv4_3.permute(0, 2, 3,\n",
    "                                      1).contiguous()  # (N, 38, 38, 16), to match prior-box order (after .view())\n",
    "        # contiguous操作需要让内存连续化，以便于进行view\n",
    "        l_conv4_3 = l_conv4_3.view(batch_size, -1, 4)  # (N, 5776, 4), there are a total 5776 boxes on this feature map\n",
    "\n",
    "        l_conv7 = self.loc_conv7(conv7_feats)  # (N, 24, 19, 19)\n",
    "        l_conv7 = l_conv7.permute(0, 2, 3, 1).contiguous()  # (N, 19, 19, 24)\n",
    "        l_conv7 = l_conv7.view(batch_size, -1, 4)  # (N, 2166, 4), there are a total 2116 boxes on this feature map\n",
    "\n",
    "        l_conv8_2 = self.loc_conv8_2(conv8_2_feats)  # (N, 24, 10, 10)\n",
    "        l_conv8_2 = l_conv8_2.permute(0, 2, 3, 1).contiguous()  # (N, 10, 10, 24)\n",
    "        l_conv8_2 = l_conv8_2.view(batch_size, -1, 4)  # (N, 600, 4)\n",
    "\n",
    "        l_conv9_2 = self.loc_conv9_2(conv9_2_feats)  # (N, 24, 5, 5)\n",
    "        l_conv9_2 = l_conv9_2.permute(0, 2, 3, 1).contiguous()  # (N, 5, 5, 24)\n",
    "        l_conv9_2 = l_conv9_2.view(batch_size, -1, 4)  # (N, 150, 4)\n",
    "\n",
    "        l_conv10_2 = self.loc_conv10_2(conv10_2_feats)  # (N, 16, 3, 3)\n",
    "        l_conv10_2 = l_conv10_2.permute(0, 2, 3, 1).contiguous()  # (N, 3, 3, 16)\n",
    "        l_conv10_2 = l_conv10_2.view(batch_size, -1, 4)  # (N, 36, 4)\n",
    "\n",
    "        l_conv11_2 = self.loc_conv11_2(conv11_2_feats)  # (N, 16, 1, 1)\n",
    "        l_conv11_2 = l_conv11_2.permute(0, 2, 3, 1).contiguous()  # (N, 1, 1, 16)\n",
    "        l_conv11_2 = l_conv11_2.view(batch_size, -1, 4)  # (N, 4, 4)\n",
    "\n",
    "        # 分类预测的分数\n",
    "        c_conv4_3 = self.cl_conv4_3(conv4_3_feats)  # (N, 4 * n_classes, 38, 38)\n",
    "        c_conv4_3 = c_conv4_3.permute(0, 2, 3,\n",
    "                                      1).contiguous()  # (N, 38, 38, 4 * n_classes)\n",
    "        c_conv4_3 = c_conv4_3.view(batch_size, -1,\n",
    "                                   self.n_classes)  # (N, 5776, n_classes)\n",
    "\n",
    "        c_conv7 = self.cl_conv7(conv7_feats)  # (N, 6 * n_classes, 19, 19)\n",
    "        c_conv7 = c_conv7.permute(0, 2, 3, 1).contiguous()  # (N, 19, 19, 6 * n_classes)\n",
    "        c_conv7 = c_conv7.view(batch_size, -1,\n",
    "                               self.n_classes)  # (N, 2166, n_classes)\n",
    "\n",
    "        c_conv8_2 = self.cl_conv8_2(conv8_2_feats)  # (N, 6 * n_classes, 10, 10)\n",
    "        c_conv8_2 = c_conv8_2.permute(0, 2, 3, 1).contiguous()  # (N, 10, 10, 6 * n_classes)\n",
    "        c_conv8_2 = c_conv8_2.view(batch_size, -1, self.n_classes)  # (N, 600, n_classes)\n",
    "\n",
    "        c_conv9_2 = self.cl_conv9_2(conv9_2_feats)  # (N, 6 * n_classes, 5, 5)\n",
    "        c_conv9_2 = c_conv9_2.permute(0, 2, 3, 1).contiguous()  # (N, 5, 5, 6 * n_classes)\n",
    "        c_conv9_2 = c_conv9_2.view(batch_size, -1, self.n_classes)  # (N, 150, n_classes)\n",
    "\n",
    "        c_conv10_2 = self.cl_conv10_2(conv10_2_feats)  # (N, 4 * n_classes, 3, 3)\n",
    "        c_conv10_2 = c_conv10_2.permute(0, 2, 3, 1).contiguous()  # (N, 3, 3, 4 * n_classes)\n",
    "        c_conv10_2 = c_conv10_2.view(batch_size, -1, self.n_classes)  # (N, 36, n_classes)\n",
    "\n",
    "        c_conv11_2 = self.cl_conv11_2(conv11_2_feats)  # (N, 4 * n_classes, 1, 1)\n",
    "        c_conv11_2 = c_conv11_2.permute(0, 2, 3, 1).contiguous()  # (N, 1, 1, 4 * n_classes)\n",
    "        c_conv11_2 = c_conv11_2.view(batch_size, -1, self.n_classes)  # (N, 4, n_classes)\n",
    "\n",
    "        # 总共8732个boxes\n",
    "        # 按照low到high的顺序连接输出\n",
    "        locs = torch.cat([l_conv4_3, l_conv7, l_conv8_2, l_conv9_2, l_conv10_2, l_conv11_2], dim=1)  # (N, 8732, 4)\n",
    "        classes_scores = torch.cat([c_conv4_3, c_conv7, c_conv8_2, c_conv9_2, c_conv10_2, c_conv11_2],\n",
    "                                   dim=1)  # (N, 8732, n_classes)\n",
    "\n",
    "        return locs, classes_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def xy_to_cxcy(xy):\n",
    "    \"\"\"\n",
    "    Convert bounding boxes from boundary coordinates (x_min, y_min, x_max, y_max) to center-size coordinates (c_x, c_y, w, h).\n",
    "    :param xy: bounding boxes in boundary coordinates, a tensor of size (n_boxes, 4)\n",
    "    :return: bounding boxes in center-size coordinates, a tensor of size (n_boxes, 4)\n",
    "    \"\"\"\n",
    "    return torch.cat([(xy[:, 2:] + xy[:, :2]) / 2,  # c_x, c_y\n",
    "                      xy[:, 2:] - xy[:, :2]], 1)  # w, h\n",
    "\n",
    "\n",
    "def cxcy_to_xy(cxcy):\n",
    "    \"\"\"\n",
    "    Convert bounding boxes from center-size coordinates (c_x, c_y, w, h) to boundary coordinates (x_min, y_min, x_max, y_max).\n",
    "    :param cxcy: bounding boxes in center-size coordinates, a tensor of size (n_boxes, 4)\n",
    "    :return: bounding boxes in boundary coordinates, a tensor of size (n_boxes, 4)\n",
    "    \"\"\"\n",
    "    return torch.cat([cxcy[:, :2] - (cxcy[:, 2:] / 2),  # x_min, y_min\n",
    "                      cxcy[:, :2] + (cxcy[:, 2:] / 2)], 1)  # x_max, y_max\n",
    "\n",
    "\n",
    "def cxcy_to_gcxgcy(cxcy, priors_cxcy):\n",
    "    \"\"\"\n",
    "    Encode bounding boxes (that are in center-size form) w.r.t. the corresponding prior boxes (that are in center-size form).\n",
    "    For the center coordinates, find the offset with respect to the prior box, and scale by the size of the prior box.\n",
    "    For the size coordinates, scale by the size of the prior box, and convert to the log-space.\n",
    "    In the model, we are predicting bounding box coordinates in this encoded form.\n",
    "    :param cxcy: bounding boxes in center-size coordinates, a tensor of size (n_priors, 4)\n",
    "    :param priors_cxcy: prior boxes with respect to which the encoding must be performed, a tensor of size (n_priors, 4)\n",
    "    :return: encoded bounding boxes, a tensor of size (n_priors, 4)\n",
    "    \"\"\"\n",
    "\n",
    "    # The 10 and 5 below are referred to as 'variances' in the original Caffe repo, completely empirical\n",
    "    # They are for some sort of numerical conditioning, for 'scaling the localization gradient'\n",
    "    # See https://github.com/weiliu89/caffe/issues/155\n",
    "    return torch.cat([(cxcy[:, :2] - priors_cxcy[:, :2]) / (priors_cxcy[:, 2:] / 10),  # g_c_x, g_c_y\n",
    "                      torch.log(cxcy[:, 2:] / priors_cxcy[:, 2:]) * 5], 1)  # g_w, g_h\n",
    "\n",
    "\n",
    "def gcxgcy_to_cxcy(gcxgcy, priors_cxcy):\n",
    "    \"\"\"\n",
    "    Decode bounding box coordinates predicted by the model, since they are encoded in the form mentioned above.\n",
    "    They are decoded into center-size coordinates.\n",
    "    This is the inverse of the function above.\n",
    "    :param gcxgcy: encoded bounding boxes, i.e. output of the model, a tensor of size (n_priors, 4)\n",
    "    :param priors_cxcy: prior boxes with respect to which the encoding is defined, a tensor of size (n_priors, 4)\n",
    "    :return: decoded bounding boxes in center-size form, a tensor of size (n_priors, 4)\n",
    "    \"\"\"\n",
    "\n",
    "    return torch.cat([gcxgcy[:, :2] * priors_cxcy[:, 2:] / 10 + priors_cxcy[:, :2],  # c_x, c_y\n",
    "                      torch.exp(gcxgcy[:, 2:] / 5) * priors_cxcy[:, 2:]], 1)  # w, h\n",
    "\n",
    "\n",
    "def find_intersection(set_1, set_2):\n",
    "    \"\"\"\n",
    "    Find the intersection of every box combination between two sets of boxes that are in boundary coordinates.\n",
    "    :param set_1: set 1, a tensor of dimensions (n1, 4)\n",
    "    :param set_2: set 2, a tensor of dimensions (n2, 4)\n",
    "    :return: intersection of each of the boxes in set 1 with respect to each of the boxes in set 2, a tensor of dimensions (n1, n2)\n",
    "    \"\"\"\n",
    "\n",
    "    # PyTorch auto-broadcasts singleton dimensions\n",
    "    lower_bounds = torch.max(set_1[:, :2].unsqueeze(1), set_2[:, :2].unsqueeze(0))  # (n1, n2, 2)\n",
    "    upper_bounds = torch.min(set_1[:, 2:].unsqueeze(1), set_2[:, 2:].unsqueeze(0))  # (n1, n2, 2)\n",
    "    intersection_dims = torch.clamp(upper_bounds - lower_bounds, min=0)  # (n1, n2, 2)\n",
    "    return intersection_dims[:, :, 0] * intersection_dims[:, :, 1]  # (n1, n2)\n",
    "\n",
    "\n",
    "def find_jaccard_overlap(set_1, set_2):\n",
    "    \"\"\"\n",
    "    Find the Jaccard Overlap (IoU) of every box combination between two sets of boxes that are in boundary coordinates.\n",
    "    :param set_1: set 1, a tensor of dimensions (n1, 4)\n",
    "    :param set_2: set 2, a tensor of dimensions (n2, 4)\n",
    "    :return: Jaccard Overlap of each of the boxes in set 1 with respect to each of the boxes in set 2, a tensor of dimensions (n1, n2)\n",
    "    \"\"\"\n",
    "\n",
    "    # Find intersections\n",
    "    intersection = find_intersection(set_1, set_2)  # (n1, n2)\n",
    "\n",
    "    # Find areas of each box in both sets\n",
    "    areas_set_1 = (set_1[:, 2] - set_1[:, 0]) * (set_1[:, 3] - set_1[:, 1])  # (n1)\n",
    "    areas_set_2 = (set_2[:, 2] - set_2[:, 0]) * (set_2[:, 3] - set_2[:, 1])  # (n2)\n",
    "\n",
    "    # Find the union\n",
    "    # PyTorch auto-broadcasts singleton dimensions\n",
    "    union = areas_set_1.unsqueeze(1) + areas_set_2.unsqueeze(0) - intersection  # (n1, n2)\n",
    "\n",
    "    return intersection / union  # (n1, n2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SSD300(nn.Module):\n",
    "    \"\"\"\n",
    "    The SSD300 network - encapsulates the base VGG network, auxiliary, and prediction convolutions.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, n_classes):\n",
    "        super(SSD300, self).__init__()\n",
    "\n",
    "        self.n_classes = n_classes\n",
    "\n",
    "        self.base = VGGBase()\n",
    "        self.aux_convs = AuxiliaryConvolutions()\n",
    "        self.pred_convs = PredictionConvolutions(n_classes)\n",
    "\n",
    "        # Since lower level features (conv4_3_feats) have considerably larger scales, we take the L2 norm and rescale\n",
    "        # Rescale factor is initially set at 20, but is learned for each channel during back-prop\n",
    "        self.rescale_factors = nn.Parameter(torch.FloatTensor(1, 512, 1, 1))  # there are 512 channels in conv4_3_feats\n",
    "        nn.init.constant_(self.rescale_factors, 20)\n",
    "\n",
    "        # Prior boxes\n",
    "        self.priors_cxcy = self.create_prior_boxes()\n",
    "\n",
    "    def forward(self, image):\n",
    "        \"\"\"\n",
    "        Forward propagation.\n",
    "        :param image: images, a tensor of dimensions (N, 3, 300, 300)\n",
    "        :return: 8732 locations and class scores (i.e. w.r.t each prior box) for each image\n",
    "        \"\"\"\n",
    "        # Run VGG base network convolutions (lower level feature map generators)\n",
    "        conv4_3_feats, conv7_feats = self.base(image)  # (N, 512, 38, 38), (N, 1024, 19, 19)\n",
    "\n",
    "        # Rescale conv4_3 after L2 norm\n",
    "        norm = conv4_3_feats.pow(2).sum(dim=1, keepdim=True).sqrt()  # (N, 1, 38, 38)\n",
    "        conv4_3_feats = conv4_3_feats / norm  # (N, 512, 38, 38)\n",
    "        conv4_3_feats = conv4_3_feats * self.rescale_factors  # (N, 512, 38, 38)\n",
    "        # (PyTorch autobroadcasts singleton dimensions during arithmetic)\n",
    "\n",
    "        # Run auxiliary convolutions (higher level feature map generators)\n",
    "        conv8_2_feats, conv9_2_feats, conv10_2_feats, conv11_2_feats = \\\n",
    "            self.aux_convs(conv7_feats)  # (N, 512, 10, 10),  (N, 256, 5, 5), (N, 256, 3, 3), (N, 256, 1, 1)\n",
    "\n",
    "        # Run prediction convolutions (predict offsets w.r.t prior-boxes and classes in each resulting localization box)\n",
    "        locs, classes_scores = self.pred_convs(conv4_3_feats, conv7_feats, conv8_2_feats, conv9_2_feats, conv10_2_feats,\n",
    "                                               conv11_2_feats)  # (N, 8732, 4), (N, 8732, n_classes)\n",
    "\n",
    "        return locs, classes_scores\n",
    "\n",
    "    def create_prior_boxes(self):\n",
    "        \"\"\"\n",
    "        Create the 8732 prior (default) boxes for the SSD300, as defined in the paper.\n",
    "        :return: prior boxes in center-size coordinates, a tensor of dimensions (8732, 4)\n",
    "        \"\"\"\n",
    "        fmap_dims = {'conv4_3': 38,\n",
    "                     'conv7': 19,\n",
    "                     'conv8_2': 10,\n",
    "                     'conv9_2': 5,\n",
    "                     'conv10_2': 3,\n",
    "                     'conv11_2': 1}\n",
    "\n",
    "        obj_scales = {'conv4_3': 0.1,\n",
    "                      'conv7': 0.2,\n",
    "                      'conv8_2': 0.375,\n",
    "                      'conv9_2': 0.55,\n",
    "                      'conv10_2': 0.725,\n",
    "                      'conv11_2': 0.9}\n",
    "\n",
    "        aspect_ratios = {'conv4_3': [1., 2., 0.5],\n",
    "                         'conv7': [1., 2., 3., 0.5, .333],\n",
    "                         'conv8_2': [1., 2., 3., 0.5, .333],\n",
    "                         'conv9_2': [1., 2., 3., 0.5, .333],\n",
    "                         'conv10_2': [1., 2., 0.5],\n",
    "                         'conv11_2': [1., 2., 0.5]}\n",
    "\n",
    "        fmaps = list(fmap_dims.keys())\n",
    "\n",
    "        prior_boxes = []\n",
    "\n",
    "        for k, fmap in enumerate(fmaps):\n",
    "            for i in range(fmap_dims[fmap]):\n",
    "                for j in range(fmap_dims[fmap]):\n",
    "                    cx = (j + 0.5) / fmap_dims[fmap]\n",
    "                    cy = (i + 0.5) / fmap_dims[fmap]\n",
    "\n",
    "                    for ratio in aspect_ratios[fmap]:\n",
    "                        prior_boxes.append([cx, cy, obj_scales[fmap] * sqrt(ratio), obj_scales[fmap] / sqrt(ratio)])\n",
    "\n",
    "                        # For an aspect ratio of 1, use an additional prior whose scale is the geometric mean of the\n",
    "                        # scale of the current feature map and the scale of the next feature map\n",
    "                        if ratio == 1.:\n",
    "                            try:\n",
    "                                additional_scale = sqrt(obj_scales[fmap] * obj_scales[fmaps[k + 1]])\n",
    "                            # For the last feature map, there is no \"next\" feature map\n",
    "                            except IndexError:\n",
    "                                additional_scale = 1.\n",
    "                            prior_boxes.append([cx, cy, additional_scale, additional_scale])\n",
    "\n",
    "        prior_boxes = torch.FloatTensor(prior_boxes).to(device)  # (8732, 4)\n",
    "        prior_boxes.clamp_(0, 1)  # (8732, 4)\n",
    "\n",
    "        return prior_boxes\n",
    "\n",
    "    def detect_objects(self, predicted_locs, predicted_scores, min_score, max_overlap, top_k):\n",
    "        \"\"\"\n",
    "        Decipher the 8732 locations and class scores (output of ths SSD300) to detect objects.\n",
    "        For each class, perform Non-Maximum Suppression (NMS) on boxes that are above a minimum threshold.\n",
    "        :param predicted_locs: predicted locations/boxes w.r.t the 8732 prior boxes, a tensor of dimensions (N, 8732, 4)\n",
    "        :param predicted_scores: class scores for each of the encoded locations/boxes, a tensor of dimensions (N, 8732, n_classes)\n",
    "        :param min_score: minimum threshold for a box to be considered a match for a certain class\n",
    "        :param max_overlap: maximum overlap two boxes can have so that the one with the lower score is not suppressed via NMS\n",
    "        :param top_k: if there are a lot of resulting detection across all classes, keep only the top 'k'\n",
    "        :return: detections (boxes, labels, and scores), lists of length batch_size\n",
    "        \"\"\"\n",
    "        batch_size = predicted_locs.size(0)\n",
    "        n_priors = self.priors_cxcy.size(0)\n",
    "        predicted_scores = F.softmax(predicted_scores, dim=2)  # (N, 8732, n_classes)\n",
    "\n",
    "        # Lists to store final predicted boxes, labels, and scores for all images\n",
    "        all_images_boxes = list()\n",
    "        all_images_labels = list()\n",
    "        all_images_scores = list()\n",
    "\n",
    "        assert n_priors == predicted_locs.size(1) == predicted_scores.size(1)\n",
    "\n",
    "        for i in range(batch_size):\n",
    "            # Decode object coordinates from the form we regressed predicted boxes to\n",
    "            decoded_locs = cxcy_to_xy(\n",
    "                gcxgcy_to_cxcy(predicted_locs[i], self.priors_cxcy))  # (8732, 4), these are fractional pt. coordinates\n",
    "\n",
    "            # Lists to store boxes and scores for this image\n",
    "            image_boxes = list()\n",
    "            image_labels = list()\n",
    "            image_scores = list()\n",
    "\n",
    "            max_scores, best_label = predicted_scores[i].max(dim=1)  # (8732)\n",
    "\n",
    "            # Check for each class\n",
    "            for c in range(1, self.n_classes):\n",
    "                # Keep only predicted boxes and scores where scores for this class are above the minimum score\n",
    "                class_scores = predicted_scores[i][:, c]  # (8732)\n",
    "                score_above_min_score = class_scores > min_score  # torch.uint8 (byte) tensor, for indexing\n",
    "                n_above_min_score = score_above_min_score.sum().item()\n",
    "                if n_above_min_score == 0:\n",
    "                    continue\n",
    "                class_scores = class_scores[score_above_min_score]  # (n_qualified), n_min_score <= 8732\n",
    "                class_decoded_locs = decoded_locs[score_above_min_score]  # (n_qualified, 4)\n",
    "\n",
    "                # Sort predicted boxes and scores by scores\n",
    "                class_scores, sort_ind = class_scores.sort(dim=0, descending=True)  # (n_qualified), (n_min_score)\n",
    "                class_decoded_locs = class_decoded_locs[sort_ind]  # (n_min_score, 4)\n",
    "\n",
    "                # Find the overlap between predicted boxes\n",
    "                overlap = find_jaccard_overlap(class_decoded_locs, class_decoded_locs)  # (n_qualified, n_min_score)\n",
    "\n",
    "                # Non-Maximum Suppression (NMS)\n",
    "\n",
    "                # A torch.uint8 (byte) tensor to keep track of which predicted boxes to suppress\n",
    "                # 1 implies suppress, 0 implies don't suppress\n",
    "                suppress = torch.zeros((n_above_min_score), dtype=torch.uint8).to(device)  # (n_qualified)\n",
    "\n",
    "                # Consider each box in order of decreasing scores\n",
    "                for box in range(class_decoded_locs.size(0)):\n",
    "                    # If this box is already marked for suppression\n",
    "                    if suppress[box] == 1:\n",
    "                        continue\n",
    "\n",
    "                    # Suppress boxes whose overlaps (with this box) are greater than maximum overlap\n",
    "                    # Find such boxes and update suppress indices\n",
    "                    suppress = torch.max(suppress, overlap[box] > max_overlap)\n",
    "                    # The max operation retains previously suppressed boxes, like an 'OR' operation\n",
    "\n",
    "                    # Don't suppress this box, even though it has an overlap of 1 with itself\n",
    "                    suppress[box] = 0\n",
    "\n",
    "                # Store only unsuppressed boxes for this class\n",
    "                image_boxes.append(class_decoded_locs[1 - suppress])\n",
    "                image_labels.append(torch.LongTensor((1 - suppress).sum().item() * [c]).to(device))\n",
    "                image_scores.append(class_scores[1 - suppress])\n",
    "\n",
    "            # If no object in any class is found, store a placeholder for 'background'\n",
    "            if len(image_boxes) == 0:\n",
    "                image_boxes.append(torch.FloatTensor([[0., 0., 1., 1.]]).to(device))\n",
    "                image_labels.append(torch.LongTensor([0]).to(device))\n",
    "                image_scores.append(torch.FloatTensor([0.]).to(device))\n",
    "\n",
    "            # Concatenate into single tensors\n",
    "            image_boxes = torch.cat(image_boxes, dim=0)  # (n_objects, 4)\n",
    "            image_labels = torch.cat(image_labels, dim=0)  # (n_objects)\n",
    "            image_scores = torch.cat(image_scores, dim=0)  # (n_objects)\n",
    "            n_objects = image_scores.size(0)\n",
    "\n",
    "            # Keep only the top k objects\n",
    "            if n_objects > top_k:\n",
    "                image_scores, sort_ind = image_scores.sort(dim=0, descending=True)\n",
    "                image_scores = image_scores[:top_k]  # (top_k)\n",
    "                image_boxes = image_boxes[sort_ind][:top_k]  # (top_k, 4)\n",
    "                image_labels = image_labels[sort_ind][:top_k]  # (top_k)\n",
    "\n",
    "            # Append to lists that store predicted boxes and scores for all images\n",
    "            all_images_boxes.append(image_boxes)\n",
    "            all_images_labels.append(image_labels)\n",
    "            all_images_scores.append(image_scores)\n",
    "\n",
    "        return all_images_boxes, all_images_labels, all_images_scores  # lists of length batch_size\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiBoxLoss(nn.Module):\n",
    "    \"\"\"\n",
    "    The MultiBox loss, a loss function for object detection.\n",
    "    This is a combination of:\n",
    "    (1) a localization loss for the predicted locations of the boxes, and\n",
    "    (2) a confidence loss for the predicted class scores.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, priors_cxcy, threshold=0.5, neg_pos_ratio=3, alpha=1.):\n",
    "        super(MultiBoxLoss, self).__init__()\n",
    "        self.priors_cxcy = priors_cxcy\n",
    "        self.priors_xy = cxcy_to_xy(priors_cxcy)\n",
    "        self.threshold = threshold\n",
    "        self.neg_pos_ratio = neg_pos_ratio\n",
    "        self.alpha = alpha\n",
    "\n",
    "        self.smooth_l1 = nn.L1Loss()\n",
    "        self.cross_entropy = nn.CrossEntropyLoss(reduce=False)\n",
    "\n",
    "    def forward(self, predicted_locs, predicted_scores, boxes, labels):\n",
    "        \"\"\"\n",
    "        Forward propagation.\n",
    "        :param predicted_locs: predicted locations/boxes w.r.t the 8732 prior boxes, a tensor of dimensions (N, 8732, 4)\n",
    "        :param predicted_scores: class scores for each of the encoded locations/boxes, a tensor of dimensions (N, 8732, n_classes)\n",
    "        :param boxes: true  object bounding boxes in boundary coordinates, a list of N tensors\n",
    "        :param labels: true object labels, a list of N tensors\n",
    "        :return: multibox loss, a scalar\n",
    "        \"\"\"\n",
    "        batch_size = predicted_locs.size(0)\n",
    "        n_priors = self.priors_cxcy.size(0)\n",
    "        n_classes = predicted_scores.size(2)\n",
    "\n",
    "        assert n_priors == predicted_locs.size(1) == predicted_scores.size(1)\n",
    "\n",
    "        true_locs = torch.zeros((batch_size, n_priors, 4), dtype=torch.float).to(device)  # (N, 8732, 4)\n",
    "        true_classes = torch.zeros((batch_size, n_priors), dtype=torch.long).to(device)  # (N, 8732)\n",
    "\n",
    "        # For each image\n",
    "        for i in range(batch_size):\n",
    "            n_objects = boxes[i].size(0)\n",
    "\n",
    "            overlap = find_jaccard_overlap(boxes[i],\n",
    "                                           self.priors_xy)  # (n_objects, 8732)\n",
    "\n",
    "            # For each prior, find the object that has the maximum overlap\n",
    "            overlap_for_each_prior, object_for_each_prior = overlap.max(dim=0)  # (8732)\n",
    "\n",
    "            # We don't want a situation where an object is not represented in our positive (non-background) priors -\n",
    "            # 1. An object might not be the best object for all priors, and is therefore not in object_for_each_prior.\n",
    "            # 2. All priors with the object may be assigned as background based on the threshold (0.5).\n",
    "\n",
    "            # To remedy this -\n",
    "            # First, find the prior that has the maximum overlap for each object.\n",
    "            _, prior_for_each_object = overlap.max(dim=1)  # (N_o)\n",
    "\n",
    "            # Then, assign each object to the corresponding maximum-overlap-prior. (This fixes 1.)\n",
    "            object_for_each_prior[prior_for_each_object] = torch.LongTensor(range(n_objects)).to(device)\n",
    "\n",
    "            # To ensure these priors qualify, artificially give them an overlap of greater than 0.5. (This fixes 2.)\n",
    "            overlap_for_each_prior[prior_for_each_object] = 1.\n",
    "\n",
    "            # Labels for each prior\n",
    "            label_for_each_prior = labels[i][object_for_each_prior]  # (8732)\n",
    "            # Set priors whose overlaps with objects are less than the threshold to be background (no object)\n",
    "            label_for_each_prior[overlap_for_each_prior < self.threshold] = 0  # (8732)\n",
    "\n",
    "            # Store\n",
    "            true_classes[i] = label_for_each_prior\n",
    "\n",
    "            # Encode center-size object coordinates into the form we regressed predicted boxes to\n",
    "            true_locs[i] = cxcy_to_gcxgcy(xy_to_cxcy(boxes[i][object_for_each_prior]), self.priors_cxcy)  # (8732, 4)\n",
    "\n",
    "        # Identify priors that are positive (object/non-background)\n",
    "        positive_priors = true_classes != 0  # (N, 8732)\n",
    "\n",
    "        # LOCALIZATION LOSS\n",
    "\n",
    "        # Localization loss is computed only over positive (non-background) priors\n",
    "        loc_loss = self.smooth_l1(predicted_locs[positive_priors], true_locs[positive_priors])  # (), scalar\n",
    "\n",
    "        # Note: indexing with a torch.uint8 (byte) tensor flattens the tensor when indexing is across multiple dimensions (N & 8732)\n",
    "        # So, if predicted_locs has the shape (N, 8732, 4), predicted_locs[positive_priors] will have (total positives, 4)\n",
    "\n",
    "        # CONFIDENCE LOSS\n",
    "\n",
    "        # Confidence loss is computed over positive priors and the most difficult (hardest) negative priors in each image\n",
    "        # That is, FOR EACH IMAGE,\n",
    "        # we will take the hardest (neg_pos_ratio * n_positives) negative priors, i.e where there is maximum loss\n",
    "        # This is called Hard Negative Mining - it concentrates on hardest negatives in each image, and also minimizes pos/neg imbalance\n",
    "\n",
    "        # Number of positive and hard-negative priors per image\n",
    "        n_positives = positive_priors.sum(dim=1)  # (N)\n",
    "        n_hard_negatives = self.neg_pos_ratio * n_positives  # (N)\n",
    "\n",
    "        # First, find the loss for all priors\n",
    "        conf_loss_all = self.cross_entropy(predicted_scores.view(-1, n_classes), true_classes.view(-1))  # (N * 8732)\n",
    "        conf_loss_all = conf_loss_all.view(batch_size, n_priors)  # (N, 8732)\n",
    "\n",
    "        # We already know which priors are positive\n",
    "        conf_loss_pos = conf_loss_all[positive_priors]  # (sum(n_positives))\n",
    "\n",
    "        # Next, find which priors are hard-negative\n",
    "        # To do this, sort ONLY negative priors in each image in order of decreasing loss and take top n_hard_negatives\n",
    "        conf_loss_neg = conf_loss_all.clone()  # (N, 8732)\n",
    "        conf_loss_neg[positive_priors] = 0.  # (N, 8732), positive priors are ignored (never in top n_hard_negatives)\n",
    "        conf_loss_neg, _ = conf_loss_neg.sort(dim=1, descending=True)  # (N, 8732), sorted by decreasing hardness\n",
    "        hardness_ranks = torch.LongTensor(range(n_priors)).unsqueeze(0).expand_as(conf_loss_neg).to(device)  # (N, 8732)\n",
    "        hard_negatives = hardness_ranks < n_hard_negatives.unsqueeze(1)  # (N, 8732)\n",
    "        conf_loss_hard_neg = conf_loss_neg[hard_negatives]  # (sum(n_hard_negatives))\n",
    "\n",
    "        # As in the paper, averaged over positive priors only, although computed over both positive and hard-negative priors\n",
    "        conf_loss = (conf_loss_hard_neg.sum() + conf_loss_pos.sum()) / n_positives.sum().float()  # (), scalar\n",
    "\n",
    "        # TOTAL LOSS\n",
    "\n",
    "        return conf_loss + self.alpha * loc_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_annotation(annotation_path):\n",
    "    tree = ET.parse(annotation_path)\n",
    "    root = tree.getroot()\n",
    "\n",
    "    boxes = list()\n",
    "    labels = list()\n",
    "    difficulties = list()\n",
    "    for object in root.iter('object'):\n",
    "\n",
    "        difficult = int(object.find('difficult').text == '1')\n",
    "\n",
    "        label = object.find('name').text.lower().strip()\n",
    "        if label not in label_map:\n",
    "            continue\n",
    "\n",
    "        bbox = object.find('bndbox')\n",
    "        xmin = int(bbox.find('xmin').text) - 1\n",
    "        ymin = int(bbox.find('ymin').text) - 1\n",
    "        xmax = int(bbox.find('xmax').text) - 1\n",
    "        ymax = int(bbox.find('ymax').text) - 1\n",
    "\n",
    "        boxes.append([xmin, ymin, xmax, ymax])\n",
    "        labels.append(label_map[label])\n",
    "        difficulties.append(difficult)\n",
    "\n",
    "    return {'boxes': boxes, 'labels': labels, 'difficulties': difficulties}\n",
    "\n",
    "\n",
    "def create_data_lists(voc07train_path, voc12train_path, voc07test_path, output_folder):\n",
    "    \"\"\"\n",
    "    Create lists of images, the bounding boxes and labels of the objects in these images, and save these to file.\n",
    "    :param voc07_path: path to the 'VOC2007' folder\n",
    "    :param voc12_path: path to the 'VOC2012' folder\n",
    "    :param output_folder: folder where the JSONs must be saved\n",
    "    \"\"\"\n",
    "    voc07_path = os.path.abspath(voc07train_path)\n",
    "    voc12_path = os.path.abspath(voc12train_path)\n",
    "\n",
    "    train_images = list()\n",
    "    train_objects = list()\n",
    "    n_objects = 0\n",
    "\n",
    "    # Training data\n",
    "    for path in [voc07train_path, voc12train_path]:\n",
    "\n",
    "        # Find IDs of images in training data\n",
    "        with open(os.path.join(path, 'ImageSets/Main/trainval.txt')) as f:\n",
    "            ids = f.read().splitlines()\n",
    "\n",
    "        for id in ids:\n",
    "            # Parse annotation's XML file\n",
    "            objects = parse_annotation(os.path.join(path, 'Annotations', id + '.xml'))\n",
    "            if len(objects['boxes']) == 0:\n",
    "                continue\n",
    "            n_objects += len(objects)\n",
    "            train_objects.append(objects)\n",
    "            train_images.append(os.path.join(path, 'JPEGImages', id + '.jpg'))\n",
    "\n",
    "    assert len(train_objects) == len(train_images)\n",
    "\n",
    "    # Save to file\n",
    "    with open(os.path.join(output_folder, 'TRAIN_images.json'), 'w') as j:\n",
    "        json.dump(train_images, j)\n",
    "    with open(os.path.join(output_folder, 'TRAIN_objects.json'), 'w') as j:\n",
    "        json.dump(train_objects, j)\n",
    "    with open(os.path.join(output_folder, 'label_map.json'), 'w') as j:\n",
    "        json.dump(label_map, j)  # save label map too\n",
    "\n",
    "    print('\\nThere are %d training images containing a total of %d objects. Files have been saved to %s.' % (\n",
    "        len(train_images), n_objects, os.path.abspath(output_folder)))\n",
    "\n",
    "    # Test data\n",
    "    test_images = list()\n",
    "    test_objects = list()\n",
    "    n_objects = 0\n",
    "\n",
    "    # Find IDs of images in the test data\n",
    "    with open(os.path.join(voc07test_path, 'ImageSets/Main/test.txt')) as f:\n",
    "        ids = f.read().splitlines()\n",
    "\n",
    "    for id in ids:\n",
    "        # Parse annotation's XML file\n",
    "        objects = parse_annotation(os.path.join(voc07test_path, 'Annotations', id + '.xml'))\n",
    "        if len(objects) == 0:\n",
    "            continue\n",
    "        test_objects.append(objects)\n",
    "        n_objects += len(objects)\n",
    "        test_images.append(os.path.join(voc07test_path, 'JPEGImages', id + '.jpg'))\n",
    "\n",
    "    assert len(test_objects) == len(test_images)\n",
    "\n",
    "    # Save to file\n",
    "    with open(os.path.join(output_folder, 'TEST_images.json'), 'w') as j:\n",
    "        json.dump(test_images, j)\n",
    "    with open(os.path.join(output_folder, 'TEST_objects.json'), 'w') as j:\n",
    "        json.dump(test_objects, j)\n",
    "\n",
    "    print('\\nThere are %d test images containing a total of %d objects. Files have been saved to %s.' % (\n",
    "        len(test_images), n_objects, os.path.abspath(output_folder)))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_data_lists_07(voc07train_path, voc07test_path, output_folder):\n",
    "    \"\"\"\n",
    "    Create lists of images, the bounding boxes and labels of the objects in these images, and save these to file.\n",
    "    :param voc07_path: path to the 'VOC2007' folder\n",
    "    :param voc12_path: path to the 'VOC2012' folder\n",
    "    :param output_folder: folder where the JSONs must be saved\n",
    "    \"\"\"\n",
    "    voc07_path = os.path.abspath(voc07train_path)\n",
    "    #voc12_path = os.path.abspath(voc12train_path)\n",
    "\n",
    "    train_images = list()\n",
    "    train_objects = list()\n",
    "    n_objects = 0\n",
    "\n",
    "    # Training data\n",
    "    for path in [voc07train_path]:\n",
    "\n",
    "        # Find IDs of images in training data\n",
    "        with open(os.path.join(path, 'ImageSets/Main/trainval.txt')) as f:\n",
    "            ids = f.read().splitlines()\n",
    "\n",
    "        for id in ids:\n",
    "            # Parse annotation's XML file\n",
    "            objects = parse_annotation(os.path.join(path, 'Annotations', id + '.xml'))\n",
    "            if len(objects['boxes']) == 0:\n",
    "                continue\n",
    "            n_objects += len(objects)\n",
    "            train_objects.append(objects)\n",
    "            train_images.append(os.path.join(path, 'JPEGImages', id + '.jpg'))\n",
    "\n",
    "    assert len(train_objects) == len(train_images)\n",
    "\n",
    "    # Save to file\n",
    "    with open(os.path.join(output_folder, 'TRAIN_images.json'), 'w') as j:\n",
    "        json.dump(train_images, j)\n",
    "    with open(os.path.join(output_folder, 'TRAIN_objects.json'), 'w') as j:\n",
    "        json.dump(train_objects, j)\n",
    "    with open(os.path.join(output_folder, 'label_map.json'), 'w') as j:\n",
    "        json.dump(label_map, j)  # save label map too\n",
    "\n",
    "    print('\\nThere are %d training images containing a total of %d objects. Files have been saved to %s.' % (\n",
    "        len(train_images), n_objects, os.path.abspath(output_folder)))\n",
    "\n",
    "    # Test data\n",
    "    test_images = list()\n",
    "    test_objects = list()\n",
    "    n_objects = 0\n",
    "\n",
    "    # Find IDs of images in the test data\n",
    "    with open(os.path.join(voc07test_path, 'ImageSets/Main/test.txt')) as f:\n",
    "        ids = f.read().splitlines()\n",
    "\n",
    "    for id in ids:\n",
    "        # Parse annotation's XML file\n",
    "        objects = parse_annotation(os.path.join(voc07test_path, 'Annotations', id + '.xml'))\n",
    "        if len(objects) == 0:\n",
    "            continue\n",
    "        test_objects.append(objects)\n",
    "        n_objects += len(objects)\n",
    "        test_images.append(os.path.join(voc07test_path, 'JPEGImages', id + '.jpg'))\n",
    "\n",
    "    assert len(test_objects) == len(test_images)\n",
    "\n",
    "    # Save to file\n",
    "    with open(os.path.join(output_folder, 'TEST_images.json'), 'w') as j:\n",
    "        json.dump(test_images, j)\n",
    "    with open(os.path.join(output_folder, 'TEST_objects.json'), 'w') as j:\n",
    "        json.dump(test_objects, j)\n",
    "\n",
    "    print('\\nThere are %d test images containing a total of %d objects. Files have been saved to %s.' % (\n",
    "        len(test_images), n_objects, os.path.abspath(output_folder)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Label map\n",
    "voc_labels = ('aeroplane', 'bicycle', 'bird', 'boat', 'bottle', 'bus', 'car', 'cat', 'chair', 'cow', 'diningtable',\n",
    "              'dog', 'horse', 'motorbike', 'person', 'pottedplant', 'sheep', 'sofa', 'train', 'tvmonitor')\n",
    "label_map = {k: v + 1 for v, k in enumerate(voc_labels)}\n",
    "label_map['background'] = 0\n",
    "rev_label_map = {v: k for k, v in label_map.items()}  # Inverse mapping\n",
    "\n",
    "# Color map for bounding boxes of detected objects from https://sashat.me/2017/01/11/list-of-20-simple-distinct-colors/\n",
    "distinct_colors = ['#e6194b', '#3cb44b', '#ffe119', '#0082c8', '#f58231', '#911eb4', '#46f0f0', '#f032e6',\n",
    "                   '#d2f53c', '#fabebe', '#008080', '#000080', '#aa6e28', '#fffac8', '#800000', '#aaffc3', '#808000',\n",
    "                   '#ffd8b1', '#e6beff', '#808080', '#FFFFFF']\n",
    "label_color_map = {k: distinct_colors[i] for i, k in enumerate(label_map.keys())}\n",
    "\n",
    "# create_data_lists(voc07train_path='/root/VOCtrainval2007/VOCtrainval_06-Nov-2007/VOCdevkit/VOC2007',                  \n",
    "#                 voc12train_path='/root/VOCtrainval2012/VOCtrainval_11-May-2012/VOCdevkit/VOC2012',\n",
    "#                 voc07test_path = '/root/VOCtest2007/VOCtest_06-Nov-2007/VOCdevkit/VOC2007',\n",
    "#                  output_folder='/root/07+12')\n",
    "# create_data_lists_07(voc07train_path='/root/VOCtrainval2007/VOCtrainval_06-Nov-2007/VOCdevkit/VOC2007',                  \n",
    "#                  voc07test_path = '/root/VOCtest2007/VOCtest_06-Nov-2007/VOCdevkit/VOC2007',\n",
    "#                  output_folder='/root/07')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 对图像数据进行增强\n",
    "def expand(image, boxes, filler):\n",
    "    '''\n",
    "    随机扩大图像到原来的最大四倍, 便于观测更小物体\n",
    "    image 为图像对应的tensor (3, h, w)\n",
    "    boxes 图像上的物体boxes (n_objects. 4)\n",
    "    filler 填充材料的RGB值 list[R, G, B]\n",
    "    \n",
    "    图像的圆点为左上角, 新image的某个区域保持原图像, 其余区域由filler决定, 分别对应与输出的三个维度\n",
    "    输出值为新的图像整体tensor和新box的原始坐标\n",
    "    '''\n",
    "    \n",
    "    original_h = image.size(1)\n",
    "    original_w = image.size(2)\n",
    "    max_scale = 4\n",
    "    scale = random.uniform(1, max_scale)\n",
    "    new_h = int(original_h*scale)\n",
    "    new_w = int(original_w*scale)\n",
    "    \n",
    "    # 初始化新图像\n",
    "    filler = torch.FloatTensor(filler)\n",
    "    new_image = torch.ones((3, new_h, new_w), dtype=torch.float) * filler.unsqueeze(1).unsqueeze(1)\n",
    "    \n",
    "    # 随机选取新图像的位置把原始图像的数据平移过来\n",
    "    left = random.randint(0, new_w - original_w)\n",
    "    right = left + original_w\n",
    "    top = random.randint(0, new_h - original_h)\n",
    "    bottom = top + original_h\n",
    "    new_image[:, top:bottom, left:right] = image\n",
    "    \n",
    "    # 原始的box做平移\n",
    "    new_boxes = boxes + torch.FloatTensor([left, top, left, top]).unsqueeze(0)\n",
    "    \n",
    "    return new_image, new_boxes\n",
    "\n",
    "def random_crop(image, boxes, labels, difficulties):\n",
    "    '''\n",
    "    对原始图像进行随机裁剪, 检测局部物体, 有可能出现原始物体被完全裁剪的情况\n",
    "    image 为图像对应的tensor (3, h, w)\n",
    "    boxes 图像上的物体boxes (n_objects. 4)\n",
    "    labels 为objects对应的label\n",
    "    difficulties objects对应的difficulty\n",
    "    '''\n",
    "    original_h = image.size(1)\n",
    "    original_w = image.size(2)\n",
    "    \n",
    "    while True:\n",
    "        # 随机选取一个最小的overlap作为box是否保留的依据, None为不裁剪\n",
    "        min_overlap = random.choice([0., .1, .3, .5, .7, .9, None])\n",
    "        if min_overlap is None:\n",
    "            return image, boxes, labels, difficulties\n",
    "        \n",
    "        # 最多进行50次裁剪尝试\n",
    "        max_trials = 50\n",
    "        for i in range(max_trials):\n",
    "            # 裁剪区间[0.3, 1]\n",
    "            min_scale = 0.3 \n",
    "            scale_h = random.uniform(min_scale, 1)\n",
    "            scale_w = random.uniform(min_scale, 1)\n",
    "            new_h = int(scale_h*original_h)\n",
    "            new_w = int(scale_w*original_w)\n",
    "            \n",
    "            # 新的长宽比要求在[0.5, 2]\n",
    "            ar = new_w/new_h\n",
    "            if not 0.5 < ar < 2:\n",
    "                continue\n",
    "            \n",
    "            # 随机选取要裁剪部分的区域坐标\n",
    "            left = random.randint(0, original_w - new_w)\n",
    "            right = left + new_w\n",
    "            top = random.randint(0, original_h - new_h)\n",
    "            bottom = top + new_h\n",
    "            crop = torch.FloatTensor([left, top, right, bottom])   # (4)\n",
    "            \n",
    "            # 计算裁剪部分和原box的交并比, 若都小于min_overlap舍弃该裁剪\n",
    "            overlap = jaccard(crop.unsqueeze(0), boxes).squeeze(0)    # (n_objects)\n",
    "            if torch.max(overlap).item() < min_overlap:\n",
    "                continue\n",
    "                \n",
    "            # 裁剪\n",
    "            new_image = image[:, top:bottom, left:right]\n",
    "            \n",
    "            # boxes中心点坐标 并看中心是否在裁剪的部分中\n",
    "            bb_centers = (boxes[:, :2]+boxes[:, 2:])/2.   # (n_objects, 2)\n",
    "            centers_in_crop = (bb_centers[:,0]>left)*(bb_centers[:,0]<right)*(bb_centers[:,1]>top)*(bb_centers[:,1]<bottom)\n",
    "            if not centers_in_crop.any():\n",
    "                continue\n",
    "            \n",
    "            new_boxes = boxes[centers_in_crop]\n",
    "            new_labels = labels[centers_in_crop]\n",
    "            new_difficulties = difficulties[centers_in_crop]\n",
    "            \n",
    "            # 只计算boxes在裁剪中的部分\n",
    "            new_boxes[:, :2] = torch.max(new_boxes[:, :2], crop[:2])\n",
    "            new_boxes[:, :2] -= crop[:2]\n",
    "            new_boxes[:, 2:] = torch.min(new_boxes[:, 2:], crop[2:])\n",
    "            new_boxes[:, 2:] -= crop[:2]\n",
    "            \n",
    "            return new_image, new_boxes, new_labels, new_difficulties\n",
    "        \n",
    "def flip(image, boxes):\n",
    "    '''\n",
    "    水平翻转图像 image为PIL\n",
    "    '''\n",
    "    # 翻转图像\n",
    "    new_image = FT.hflip(image)\n",
    "    \n",
    "    # 翻转boxes\n",
    "    new_boxes = boxes\n",
    "    new_boxes[:, 0] = image.width - boxes[:, 0] - 1\n",
    "    new_boxes[:, 2] = image.width - boxes[:, 2] - 1\n",
    "    new_boxes = new_boxes[:, [2, 1, 0, 3]]\n",
    "\n",
    "    return new_image, new_boxes\n",
    "\n",
    "\n",
    "def resize(image, boxes, dims=(300, 300), return_percent_coords=True):\n",
    "    '''\n",
    "    变换图像的size, 默认变成(300, 300), 默认保留bounding boxes的百分比坐标\n",
    "    输入为原图像(PIL)及其boxes的原始坐标, 输出为更新的图像和坐标\n",
    "    '''\n",
    "    \n",
    "    # 图像的resize\n",
    "    new_image = FT.resize(image, dims)\n",
    "        \n",
    "    # boxes的resize\n",
    "    old_dims = torch.FloatTensor([image.width, image.height, image.width, image.height])\n",
    "    new_boxes = boxes/old_dims\n",
    "    \n",
    "    if not return_percent_coords:\n",
    "        new_dims = torch.FloatTensor([dims[1], dims[0], dims[1], dims[0]]).unsqueeze(0)\n",
    "        new_boxes = new_boxes * new_dims\n",
    "\n",
    "    return new_image, new_boxes\n",
    "\n",
    "def photometric_distort(image):\n",
    "    \"\"\"\n",
    "    扭曲亮度，对比度，饱和度和色调，每一个都有50%的机会，在随机顺序\n",
    "    :param image: image, a PIL Image\n",
    "    :return: distorted image\n",
    "    \"\"\"\n",
    "    new_image = image\n",
    "\n",
    "    distortions = [FT.adjust_brightness,\n",
    "                   FT.adjust_contrast,\n",
    "                   FT.adjust_saturation,\n",
    "                   FT.adjust_hue]\n",
    "\n",
    "    random.shuffle(distortions)\n",
    "\n",
    "    for d in distortions:\n",
    "        if random.random() < 0.5:\n",
    "            if d.__name__ is 'adjust_hue':\n",
    "                # Caffe repo uses a 'hue_delta' of 18 - we divide by 255 because PyTorch needs a normalized value\n",
    "                adjust_factor = random.uniform(-18 / 255., 18 / 255.)\n",
    "            else:\n",
    "                # Caffe repo uses 'lower' and 'upper' values of 0.5 and 1.5 for brightness, contrast, and saturation\n",
    "                adjust_factor = random.uniform(0.5, 1.5)\n",
    "\n",
    "            # Apply this distortion\n",
    "            new_image = d(new_image, adjust_factor)\n",
    "\n",
    "    return new_image\n",
    "\n",
    "\n",
    "def transform(image, boxes, labels, difficulties, split):\n",
    "    '''\n",
    "    对图像进行综合转化\n",
    "    image: PIL image\n",
    "    boxes: 边界框的原始坐标 (n_objects, 4)\n",
    "    labels: objects的标签 (n_objects)\n",
    "    difficulties: 检测目标是否困难 (n_objects)\n",
    "    split: 'TRAIN' or 'TEST' 不同的过程进行不同的transform\n",
    "    '''\n",
    "    assert split in {'TRAIN', 'TEST'}\n",
    "    \n",
    "    # Mean and standard deviation of ImageNet data that our base VGG from torchvision was trained on\n",
    "    # see: https://pytorch.org/docs/stable/torchvision/models.html\n",
    "    mean = [0.485, 0.456, 0.406]\n",
    "    std = [0.229, 0.224, 0.225]\n",
    "    \n",
    "    new_image = image\n",
    "    new_boxes = boxes\n",
    "    new_labels = labels\n",
    "    new_difficulties = difficulties\n",
    "    \n",
    "    if split == 'TRAIN':\n",
    "        new_image = photometric_distort(new_image)    #0.5的概率进行\n",
    "        new_image = FT.to_tensor(new_image)\n",
    "        \n",
    "        # 以0.5的概率扩大图像\n",
    "        if random.random() < 0.5:\n",
    "            new_image, new_boxes = expand(new_image, boxes, filler=mean)\n",
    "        \n",
    "        # 裁剪\n",
    "        new_image, new_boxes, new_labels, new_difficulties = random_crop(new_image, new_boxes, new_labels,\n",
    "                                                                         new_difficulties)\n",
    "        \n",
    "        # 转回原格式\n",
    "        new_image = FT.to_pil_image(new_image)\n",
    "        \n",
    "        # 以0.5的概率旋转\n",
    "        if random.random() < 0.5:\n",
    "            new_image, new_boxes = flip(new_image, new_boxes)\n",
    "    \n",
    "    # 对图像进行resize 到(300, 300) 并把box的坐标转化成百分比形式\n",
    "    new_image, new_boxes = resize(new_image, new_boxes, dims = (300,300))\n",
    "    \n",
    "    # 转化成tensor并进行normalize\n",
    "    new_image = FT.to_tensor(new_image)\n",
    "    new_image = FT.normalize(new_image, mean=mean, std=std)\n",
    "    \n",
    "    return new_image, new_boxes, new_labels, new_difficulties\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PascalVOCDataset(Dataset):\n",
    "    '''\n",
    "    创建数据批训练\n",
    "    '''\n",
    "    def __init__(self, data_folder, split, keep_difficult=False):\n",
    "        '''输入参数\n",
    "        data_folder: 存放数据的文件夹\n",
    "        split: TRAIN或者TEST\n",
    "        keep_difficult: 是否保留难以检测的物体\n",
    "        '''\n",
    "        \n",
    "        self.split = split.upper()\n",
    "        assert self.split in {'TRAIN', 'TEST'}\n",
    "        \n",
    "        self.data_folder = data_folder\n",
    "        self.keep_difficult = keep_difficult\n",
    "        \n",
    "        # 读取数据文件\n",
    "        with open(os.path.join(data_folder, self.split + '_images.json'), 'r') as j:\n",
    "            self.images = json.load(j)\n",
    "        with open(os.path.join(data_folder, self.split + '_objects.json'), 'r') as j:\n",
    "            self.objects = json.load(j)\n",
    "\n",
    "        assert len(self.images) == len(self.objects)\n",
    "        \n",
    "        \n",
    "    def __getitem__(self, i):\n",
    "        \n",
    "        # 读取图像\n",
    "        image = Image.open(self.images[i], mode='r')\n",
    "        image = image.convert('RGB')\n",
    "        \n",
    "        # 读取图像中的物体 (bounding boxes, labels, difficulties)\n",
    "        objects = self.objects[i]\n",
    "        boxes = torch.FloatTensor(objects['boxes'])   # (n_objects, 4)\n",
    "        labels = torch.LongTensor(objects['labels'])  # (n_objects)\n",
    "        difficulties = torch.ByteTensor(objects['difficulties'])\n",
    "        \n",
    "        if not self.keep_difficult:\n",
    "            boxes = boxes[1 - difficulties]\n",
    "            labels = labels[1 - difficulties]\n",
    "            difficulties = difficulties[1 - difficulties]\n",
    "        \n",
    "        # 做transform\n",
    "        image, boxes, labels, difficulties = transform(image, boxes, labels, difficulties, split=self.split)\n",
    "\n",
    "        return image, boxes, labels, difficulties\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "    \n",
    "    def collate_fn(self, batch):\n",
    "        '''不同image可能有不一样的物体数量, 需要一个函数处理不同数量进而传递给Dataloader. \n",
    "        输入变量: __getitem__()输出的 an iterable of N sets\n",
    "        输出: image_tensor和不同size的tensor组成的bounding boxes，labels以及difficulties列表, 列表共有N个元素\n",
    "        '''\n",
    "        images = list()\n",
    "        boxes = list()\n",
    "        labels = list()\n",
    "        difficulties = list()\n",
    "\n",
    "        for b in batch:\n",
    "            images.append(b[0])\n",
    "            boxes.append(b[1])\n",
    "            labels.append(b[2])\n",
    "            difficulties.append(b[3])\n",
    "\n",
    "        images = torch.stack(images, dim=0)\n",
    "\n",
    "        return images, boxes, labels, difficulties  # tensor (N, 3, 300, 300), 3 lists of N tensors each"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_mAP(det_boxes, det_labels, det_scores, true_boxes, true_labels, true_difficulties):\n",
    "    ''' 输入均为由tensor组成的列表, 每个tensor代表一个image\n",
    "    det_boxes 训练检测出来的boxes坐标\n",
    "    det_labels 训练检测出的boxes对应的label\n",
    "    det_scores 训练检测出的boxes对应以上label的scores，用来设定阈值\n",
    "    true_boxes 真实boxes坐标\n",
    "    true_labels 真实label\n",
    "    true_difficulties 真实的difficulties\n",
    "    输出为每一类对应的AP, mAP\n",
    "    '''\n",
    "    \n",
    "    # label map为每一类机器对应标号, 共21个(1+20)\n",
    "    assert len(det_boxes) == len(det_labels) == len(det_scores) == len(true_boxes) == len(\n",
    "        true_labels) == len(\n",
    "        true_difficulties)  \n",
    "    n_classes = len(label_map)\n",
    "    \n",
    "    # 合并数据, 并记下每条数据对应的图片, 真实和检测值都需要\n",
    "    true_images = []\n",
    "    for i in range(len(true_labels)):\n",
    "        true_images.extend([i]*true_labels[i].size(0))\n",
    "    true_images = torch.LongTensor(true_images).to(device)\n",
    "    true_boxes = torch.cat(true_boxes, dim=0)  # (n_objects, 4)\n",
    "    true_labels = torch.cat(true_labels, dim=0)  # (n_objects)\n",
    "    true_difficulties = torch.cat(true_difficulties, dim=0)  # (n_objects)\n",
    "\n",
    "    assert true_images.size(0) == true_boxes.size(0) == true_labels.size(0)\n",
    "\n",
    "    det_images = list()\n",
    "    for i in range(len(det_labels)):\n",
    "        det_images.extend([i] * det_labels[i].size(0))\n",
    "    det_images = torch.LongTensor(det_images).to(device)  # (n_detections)\n",
    "    det_boxes = torch.cat(det_boxes, dim=0)  # (n_detections, 4)\n",
    "    det_labels = torch.cat(det_labels, dim=0)  # (n_detections)\n",
    "    det_scores = torch.cat(det_scores, dim=0)  # (n_detections)\n",
    "\n",
    "    assert det_images.size(0) == det_boxes.size(0) == det_labels.size(0) == det_scores.size(0)\n",
    "    \n",
    "    # 对每一类(除了背景)计算AP\n",
    "    average_precisions = torch.zeros((n_classes-1), dtype=torch.float) # (n_classes)\n",
    "    for c in range(1, n_classes):\n",
    "        # 只计算当前类对应的object, 真实的和检测的\n",
    "        true_class_images = true_images[true_labels == c]  # (n_class_objects)\n",
    "        true_class_boxes = true_boxes[true_labels == c] # (n_class_objects, 4)\n",
    "        true_class_difficulties = true_difficulties[true_labels == c] # (n_class_objects)\n",
    "        n_easy_class_objects = (1-true_class_difficulties).sum().item()\n",
    "        \n",
    "        true_class_boxes_detected = torch.zeros((true_class_difficulties.size(0)), dtype=torch.uint8).to(device)  # (n_class_objects)\n",
    "        \n",
    "        det_class_images = det_images[det_labels == c]  # (n_class_detections)\n",
    "        det_class_boxes = det_boxes[det_labels == c]  # (n_class_detections, 4)\n",
    "        det_class_scores = det_scores[det_labels == c]  # (n_class_detections)\n",
    "        n_class_detections = det_class_boxes.size(0)\n",
    "        if n_class_detections == 0:\n",
    "            continue\n",
    "            \n",
    "        # 对检测结果的分数进行排序, 以方便设置阈值\n",
    "        det_class_scores, sort_id = torch.sort(det_class_scores, dim=0, descending=True)\n",
    "        det_class_images = det_class_images[sort_id]\n",
    "        def_class_boxes = det_class_boxes[sort_id]\n",
    "        \n",
    "        # 对检测为True的看他时TP还是FP\n",
    "        true_positives = torch.zeros((n_class_detections), dtype=torch.float).to(device)  # (n_class_detections)\n",
    "        false_positives = torch.zeros((n_class_detections), dtype=torch.float).to(device)  # (n_class_detections)\n",
    "        # 对检测出来的每个box对其进行真假判定\n",
    "        for d in range(n_class_detections):\n",
    "            this_detection_box = det_class_boxes[d].unsqueeze(0)   # (1,4)\n",
    "            this_image = det_class_images[d]\n",
    "            \n",
    "            object_boxes = true_class_boxes[true_class_images == this_image]\n",
    "            object_difficulties = true_class_difficulties[true_class_images == this_image]       # (n_class_objects_in_img)\n",
    "            \n",
    "            if object_boxes.size(0) == 0:     # 预测类错误\n",
    "                false_positives[d] = 1\n",
    "                continue\n",
    "            # 检查二者的overlap\n",
    "            overlap = jaccard(this_detection_box, object_boxes)  # (1, n_class_object_in_img)\n",
    "            max_overlap, ind = (overlap.squeeze(0)).max(dim=0)\n",
    "            \n",
    "            # 检查对应的box是否已经被检测到\n",
    "            original_ind = torch.LongTensor(range(true_class_boxes.size(0)))[true_class_images == this_image][ind]\n",
    "            \n",
    "            # 阈值为0.5，为TP的条件是大于阈值，不difficult且在真实box还没有被检测过\n",
    "            if max_overlap.item()>0.5:\n",
    "                if object_difficulties[ind] == 0:\n",
    "                    if true_class_boxes_detected[original_ind] == 0:\n",
    "                        true_positives[d] = 1\n",
    "                        true_class_boxes_detected[original_ind] = 1\n",
    "                    else:\n",
    "                        false_positives[d] = 1\n",
    "            else:\n",
    "                false_positives[d] = 1\n",
    "            \n",
    "        # 按照分数递减的顺序计算累计precision和recall\n",
    "        cumul_true_positives = torch.cumsum(true_positives, dim=0)\n",
    "        cumul_false_positives = torch.cumsum(false_positives, dim=0)\n",
    "        cumul_precisions = cumul_true_positives / (cumul_true_positives+cumul_false_positives+1e-10)\n",
    "        cumul_recall = cumul_true_positives / n_easy_class_objects\n",
    "        \n",
    "        # 在不同的recall阈值下找对应的precision最大值\n",
    "        recalls_thresholds = torch.arange(start=0, end=1.1, step=0.1).tolist()\n",
    "        precisions = torch.zeros((len(recalls_thresholds)), dtype=torch.float).to(device)  # (11)\n",
    "        for i, t in enumerate(recalls_thresholds):\n",
    "            recalls_above_t = cumul_recall > t\n",
    "            if recalls_above_t.any():\n",
    "                precisions[i] = cumul_precisions[recalls_above_t].max()\n",
    "            else:\n",
    "                precisions[i] = 0\n",
    "        aversge_precisions[c-1] = precisions.mean()   # c in [1, n_classes-1]\n",
    "    \n",
    "    mAP = aversge_precisions.mean().item()\n",
    "    \n",
    "    # 存成字典形式，加入label\n",
    "    AP = {rev_label_map[c+1]:v for c, v in enumerate(average_precisions.tolist())}\n",
    "    \n",
    "    return AP, mAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def adjust_learning_rate(optimizer, scale):\n",
    "    \"\"\"\n",
    "    以scale因子调整学习率\n",
    "    :param optimizer: optimizer whose learning rate must be shrunk.\n",
    "    :param scale: factor to multiply learning rate with.\n",
    "    \"\"\"\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group['lr'] = param_group['lr'] * scale\n",
    "    print(\"DECAYING learning rate.\\n The new LR is %f\\n\" % (optimizer.param_groups[1]['lr'],))\n",
    "\n",
    "def accuracy(scores, targets, k):\n",
    "    \"\"\"\n",
    "    Computes top-k accuracy, from predicted and true labels.\n",
    "    :param scores: scores from the model\n",
    "    :param targets: true labels\n",
    "    :param k: k in top-k accuracy\n",
    "    :return: top-k accuracy\n",
    "    \"\"\"\n",
    "    batch_size = targets.size(0)\n",
    "    _, ind = scores.topk(k, 1, True, True)\n",
    "    correct = ind.eq(targets.view(-1, 1).expand_as(ind))\n",
    "    correct_total = correct.view(-1).float().sum()  # 0D tensor\n",
    "    return correct_total.item() * (100.0 / batch_size)\n",
    "\n",
    "\n",
    "def save_checkpoint(epoch, model, optimizer):\n",
    "    \"\"\"\n",
    "    Save model checkpoint.\n",
    "    :param epoch: epoch number\n",
    "    :param model: model\n",
    "    :param optimizer: optimizer\n",
    "    \"\"\"\n",
    "    state = {'epoch': epoch,\n",
    "             'model': model,\n",
    "             'optimizer': optimizer}\n",
    "    filename = 'checkpoint_ssd300.pth.tar'\n",
    "    torch.save(state, filename)\n",
    "\n",
    "\n",
    "class AverageMeter(object):\n",
    "    \"\"\"\n",
    "    Keeps track of most recent, average, sum, and count of a metric.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count\n",
    "\n",
    "\n",
    "def clip_gradient(optimizer, grad_clip):\n",
    "    \"\"\"\n",
    "    Clips gradients computed during backpropagation to avoid explosion of gradients.\n",
    "    :param optimizer: optimizer with the gradients to be clipped\n",
    "    :param grad_clip: clip value\n",
    "    \"\"\"\n",
    "    for group in optimizer.param_groups:\n",
    "        for param in group['params']:\n",
    "            if param.grad is not None:\n",
    "                param.grad.data.clamp_(-grad_clip, grad_clip)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 训练参数设置\n",
    "data_folder = 'E:/数据集/PASCAL_VOC/07'\n",
    "keep_difficult = True\n",
    "n_classes = len(label_map)\n",
    "\n",
    "checkpoint = 'checkpoint_ssd300_1.pth.tar'\n",
    "batch_size = 32\n",
    "iterations = 120000\n",
    "workers = 0\n",
    "print_freq = 200\n",
    "lr = 1e-3\n",
    "decay_lr_at = [80000, 100000] \n",
    "decay_lr_to = 0.1  \n",
    "momentum = 0.9 \n",
    "weight_decay = 5e-4  \n",
    "grad_clip = None\n",
    "import torch.backends.cudnn as cudnn\n",
    "cudnn.benchmark = True\n",
    "# 将会让程序在开始时花费一点额外时间，为整个网络的每个卷积层搜索最适合它的卷积实现算法，进而实现网络的加速"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loaded checkpoint from epoch 45.\n",
      "\n",
      "Epoch: [45][0/157]\tBatch Time 32.592 (32.592)\tData Time 3.306 (3.306)\tLoss 2.6488 (2.6488)\t\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-25-3ae77d61738c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    126\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    127\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'__main__'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 128\u001b[1;33m     \u001b[0mmain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-25-3ae77d61738c>\u001b[0m in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m     58\u001b[0m               \u001b[0mcriterion\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcriterion\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     59\u001b[0m               \u001b[0moptimizer\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 60\u001b[1;33m               epoch=epoch)\n\u001b[0m\u001b[0;32m     61\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     62\u001b[0m         \u001b[1;31m# Save checkpoint\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-25-3ae77d61738c>\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(train_loader, model, criterion, optimizer, epoch)\u001b[0m\n\u001b[0;32m     91\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     92\u001b[0m         \u001b[1;31m# Forward prop.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 93\u001b[1;33m         \u001b[0mpredicted_locs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpredicted_scores\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# (N, 8732, 4), (N, 8732, n_classes)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     94\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     95\u001b[0m         \u001b[1;31m# Loss\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32me:\\miniconda\\miniconda\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    888\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 889\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[0;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-7-8139aa3ff87b>\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, image)\u001b[0m\n\u001b[0;32m     28\u001b[0m         \"\"\"\n\u001b[0;32m     29\u001b[0m         \u001b[1;31m# Run VGG base network convolutions (lower level feature map generators)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 30\u001b[1;33m         \u001b[0mconv4_3_feats\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconv7_feats\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbase\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# (N, 512, 38, 38), (N, 1024, 19, 19)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     31\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     32\u001b[0m         \u001b[1;31m# Rescale conv4_3 after L2 norm\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32me:\\miniconda\\miniconda\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    888\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 889\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[0;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-3-8953b22b45ad>\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, image)\u001b[0m\n\u001b[0;32m     59\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     60\u001b[0m         \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconv4_1\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# (N, 512, 38, 38)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 61\u001b[1;33m         \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconv4_2\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# (N, 512, 38, 38)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     62\u001b[0m         \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconv4_3\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# (N, 512, 38, 38)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     63\u001b[0m         \u001b[0mconv4_3_feats\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mout\u001b[0m  \u001b[1;31m# (N, 512, 38, 38)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32me:\\miniconda\\miniconda\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    888\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 889\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[0;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32me:\\miniconda\\miniconda\\lib\\site-packages\\torch\\nn\\modules\\conv.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    397\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    398\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 399\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_conv_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    400\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    401\u001b[0m \u001b[1;32mclass\u001b[0m \u001b[0mConv3d\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_ConvNd\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32me:\\miniconda\\miniconda\\lib\\site-packages\\torch\\nn\\modules\\conv.py\u001b[0m in \u001b[0;36m_conv_forward\u001b[1;34m(self, input, weight, bias)\u001b[0m\n\u001b[0;32m    394\u001b[0m                             _pair(0), self.dilation, self.groups)\n\u001b[0;32m    395\u001b[0m         return F.conv2d(input, weight, bias, self.stride,\n\u001b[1;32m--> 396\u001b[1;33m                         self.padding, self.dilation, self.groups)\n\u001b[0m\u001b[0;32m    397\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    398\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    \"\"\"\n",
    "    Training.\n",
    "    \"\"\"\n",
    "    global start_epoch, label_map, epoch, checkpoint, decay_lr_at\n",
    "\n",
    "    # Initialize model or load checkpoint\n",
    "    if checkpoint is None:\n",
    "        start_epoch = 0\n",
    "        model = SSD300(n_classes=n_classes)\n",
    "        # Initialize the optimizer, with twice the default learning rate for biases, as in the original Caffe repo\n",
    "        biases = list()\n",
    "        not_biases = list()\n",
    "        for param_name, param in model.named_parameters():\n",
    "            if param.requires_grad:\n",
    "                if param_name.endswith('.bias'):\n",
    "                    biases.append(param)\n",
    "                else:\n",
    "                    not_biases.append(param)\n",
    "        optimizer = torch.optim.SGD(params=[{'params': biases, 'lr': 2 * lr}, {'params': not_biases}],\n",
    "                                    lr=lr, momentum=momentum, weight_decay=weight_decay)\n",
    "\n",
    "    else:\n",
    "        checkpoint = torch.load(checkpoint)\n",
    "        start_epoch = checkpoint['epoch'] + 1\n",
    "        print('\\nLoaded checkpoint from epoch %d.\\n' % start_epoch)\n",
    "        model = checkpoint['model']\n",
    "        optimizer = checkpoint['optimizer']\n",
    "\n",
    "    # Move to default device\n",
    "    model = model.to(device)\n",
    "    criterion = MultiBoxLoss(priors_cxcy=model.priors_cxcy).to(device)\n",
    "\n",
    "    # Custom dataloaders\n",
    "    train_dataset = PascalVOCDataset(data_folder,\n",
    "                                     split='train',\n",
    "                                     keep_difficult=keep_difficult)\n",
    "    train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True,\n",
    "                                               collate_fn=train_dataset.collate_fn, num_workers=workers,\n",
    "                                               pin_memory=True)  # note that we're passing the collate function here\n",
    "\n",
    "    # Calculate total number of epochs to train and the epochs to decay learning rate at (i.e. convert iterations to epochs)\n",
    "    # To convert iterations to epochs, divide iterations by the number of iterations per epoch\n",
    "    # The paper trains for 120,000 iterations with a batch size of 32, decays after 80,000 and 100,000 iterations\n",
    "    epochs = iterations // (len(train_dataset) // 32)\n",
    "    decay_lr_at = [it // (len(train_dataset) // 32) for it in decay_lr_at]\n",
    "\n",
    "    # Epochs\n",
    "    for epoch in range(start_epoch, epochs):\n",
    "\n",
    "        # Decay learning rate at particular epochs\n",
    "        if epoch in decay_lr_at:\n",
    "            adjust_learning_rate(optimizer, decay_lr_to)\n",
    "\n",
    "        # One epoch's training\n",
    "        train(train_loader=train_loader,\n",
    "              model=model,\n",
    "              criterion=criterion,\n",
    "              optimizer=optimizer,\n",
    "              epoch=epoch)\n",
    "\n",
    "        # Save checkpoint\n",
    "        save_checkpoint(epoch, model, optimizer)\n",
    "\n",
    "\n",
    "def train(train_loader, model, criterion, optimizer, epoch):\n",
    "    \"\"\"\n",
    "    One epoch's training.\n",
    "    :param train_loader: DataLoader for training data\n",
    "    :param model: model\n",
    "    :param criterion: MultiBox loss\n",
    "    :param optimizer: optimizer\n",
    "    :param epoch: epoch number\n",
    "    \"\"\"\n",
    "    model.train()  # training mode enables dropout\n",
    "\n",
    "    batch_time = AverageMeter()  # forward prop. + back prop. time\n",
    "    data_time = AverageMeter()  # data loading time\n",
    "    losses = AverageMeter()  # loss\n",
    "\n",
    "    start = time.time()\n",
    "\n",
    "    # Batches\n",
    "    for i, (images, boxes, labels, _) in enumerate(train_loader):\n",
    "        data_time.update(time.time() - start)\n",
    "\n",
    "        # Move to default device\n",
    "        images = images.to(device)  # (batch_size (N), 3, 300, 300)\n",
    "        boxes = [b.to(device) for b in boxes]\n",
    "        labels = [l.to(device) for l in labels]\n",
    "\n",
    "        # Forward prop.\n",
    "        predicted_locs, predicted_scores = model(images)  # (N, 8732, 4), (N, 8732, n_classes)\n",
    "\n",
    "        # Loss\n",
    "        loss = criterion(predicted_locs, predicted_scores, boxes, labels)  # scalar\n",
    "\n",
    "        # Backward prop.\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "\n",
    "        # Clip gradients, if necessary\n",
    "        if grad_clip is not None:\n",
    "            clip_gradient(optimizer, grad_clip)\n",
    "\n",
    "        # Update model\n",
    "        optimizer.step()\n",
    "\n",
    "        losses.update(loss.item(), images.size(0))\n",
    "        batch_time.update(time.time() - start)\n",
    "\n",
    "        start = time.time()\n",
    "\n",
    "        # Print status\n",
    "        if i % print_freq == 0:\n",
    "            print('Epoch: [{0}][{1}/{2}]\\t'\n",
    "                  'Batch Time {batch_time.val:.3f} ({batch_time.avg:.3f})\\t'\n",
    "                  'Data Time {data_time.val:.3f} ({data_time.avg:.3f})\\t'\n",
    "                  'Loss {loss.val:.4f} ({loss.avg:.4f})\\t'.format(epoch, i, len(train_loader),\n",
    "                                                                  batch_time=batch_time,\n",
    "                                                      data_time=data_time, loss=losses))\n",
    "        # if i % 20 == 0:\n",
    "        #    save_checkpoint2(epoch, model, optimizer)   \n",
    "    del predicted_locs, predicted_scores, images, boxes, labels  # free some memory since their histories may be stored\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "checkpoint = 'checkpoint_ssd300_1.pth.tar'\n",
    "data_folder = 'E:/数据集/PASCAL_VOC/07'\n",
    "# Load model checkpoint that is to be evaluated\n",
    "checkpoint = torch.load(checkpoint)\n",
    "model = checkpoint['model']\n",
    "model = model.to(device)\n",
    "\n",
    "model.eval()\n",
    "test_dataset = PascalVOCDataset(data_folder,\n",
    "                                split='test',\n",
    "                                keep_difficult=keep_difficult)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=False,\n",
    "                                          collate_fn=test_dataset.collate_fn, num_workers=0, pin_memory=True)\n",
    "\n",
    "def evaluate(test_loader, model):\n",
    "    \"\"\"\n",
    "    Evaluate.\n",
    "    :param test_loader: DataLoader for test data\n",
    "    :param model: model\n",
    "    \"\"\"\n",
    "\n",
    "    # Make sure it's in eval mode\n",
    "    model.eval()\n",
    "\n",
    "    # Lists to store detected and true boxes, labels, scores\n",
    "    det_boxes = list()\n",
    "    det_labels = list()\n",
    "    det_scores = list()\n",
    "    true_boxes = list()\n",
    "    true_labels = list()\n",
    "    true_difficulties = list()  # it is necessary to know which objects are 'difficult', see 'calculate_mAP' in utils.py\n",
    "\n",
    "    with torch.no_grad():\n",
    "        # Batches\n",
    "        for i, (images, boxes, labels, difficulties) in enumerate(tqdm(test_loader, desc='Evaluating')):\n",
    "            images = images.to(device)  # (N, 3, 300, 300)\n",
    "            print(i)\n",
    "\n",
    "            # Forward prop.\n",
    "            predicted_locs, predicted_scores = model(images)\n",
    "\n",
    "            # Detect objects in SSD output\n",
    "            det_boxes_batch, det_labels_batch, det_scores_batch = model.detect_objects(predicted_locs, predicted_scores,\n",
    "                                                                                       min_score=0.01, max_overlap=0.45,\n",
    "                                                                                       top_k=200)\n",
    "            # Evaluation MUST be at min_score=0.01, max_overlap=0.45, top_k=200 for fair comparision with the paper's results and other repos\n",
    "\n",
    "            # Store this batch's results for mAP calculation\n",
    "            boxes = [b.to(device) for b in boxes]\n",
    "            labels = [l.to(device) for l in labels]\n",
    "            difficulties = [d.to(device) for d in difficulties]\n",
    "\n",
    "            det_boxes.extend(det_boxes_batch)\n",
    "            det_labels.extend(det_labels_batch)\n",
    "            det_scores.extend(det_scores_batch)\n",
    "            true_boxes.extend(boxes)\n",
    "            true_labels.extend(labels)\n",
    "            true_difficulties.extend(difficulties)\n",
    "\n",
    "        # Calculate mAP\n",
    "        torch.save(det_boxes, 'det_boxes')\n",
    "        torch.save(det_labels, 'det_labels')\n",
    "        torch.save(det_scores, 'det_scores')\n",
    "        torch.save(true_boxes, 'true_boxes')\n",
    "        torch.save(true_labels, 'true_labels')\n",
    "        torch.save(true_difficulties, 'true_difficulties')\n",
    "        APs, mAP = calculate_mAP(det_boxes, det_labels, det_scores, true_boxes, true_labels, true_difficulties)\n",
    "\n",
    "    # Print AP for each class\n",
    "    pp.pprint(APs)\n",
    "\n",
    "    print('\\nMean Average Precision (mAP): %.3f' % mAP)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    evaluate(test_loader, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Load model checkpoint\n",
    "checkpoint = 'checkpoint_ssd300_1.pth.tar'\n",
    "checkpoint = torch.load(checkpoint)\n",
    "start_epoch = checkpoint['epoch'] + 1\n",
    "print('\\nLoaded checkpoint from epoch %d.\\n' % start_epoch)\n",
    "model = checkpoint['model']\n",
    "model = model.to(device)\n",
    "model.eval()\n",
    "# Transforms\n",
    "\n",
    "resize = transforms.Resize((300, 300))\n",
    "to_tensor = transforms.ToTensor()\n",
    "normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                 std=[0.229, 0.224, 0.225])\n",
    "\n",
    "\n",
    "def detect(original_image, min_score, max_overlap, top_k, suppress=None):\n",
    "    \"\"\"\n",
    "    Detect objects in an image with a trained SSD300, and visualize the results.\n",
    "    :param original_image: image, a PIL Image\n",
    "    :param min_score: minimum threshold for a detected box to be considered a match for a certain class\n",
    "    :param max_overlap: maximum overlap two boxes can have so that the one with the lower score is not suppressed via Non-Maximum Suppression (NMS)\n",
    "    :param top_k: if there are a lot of resulting detection across all classes, keep only the top 'k'\n",
    "    :param suppress: classes that you know for sure cannot be in the image or you do not want in the image, a list\n",
    "    :return: annotated image, a PIL Image\n",
    "    \"\"\"\n",
    "\n",
    "    # Transform\n",
    "    image = normalize(to_tensor(resize(original_image)))\n",
    "\n",
    "    # Move to default device\n",
    "    image = image.to(device)\n",
    "\n",
    "    # Forward prop.\n",
    "    predicted_locs, predicted_scores = model(image.unsqueeze(0))\n",
    "\n",
    "    # Detect objects in SSD output\n",
    "    det_boxes, det_labels, det_scores = model.detect_objects(predicted_locs, predicted_scores, min_score=min_score,\n",
    "                                                             max_overlap=max_overlap, top_k=top_k)\n",
    "\n",
    "    # Move detections to the CPU\n",
    "    det_boxes = det_boxes[0].to('cpu')\n",
    "\n",
    "    # Transform to original image dimensions\n",
    "    original_dims = torch.FloatTensor(\n",
    "        [original_image.width, original_image.height, original_image.width, original_image.height]).unsqueeze(0)\n",
    "    det_boxes = det_boxes * original_dims\n",
    "\n",
    "    # Decode class integer labels\n",
    "    det_labels = [rev_label_map[l] for l in det_labels[0].to('cpu').tolist()]\n",
    "\n",
    "    # If no objects found, the detected labels will be set to ['0.'], i.e. ['background'] in SSD300.detect_objects() in model.py\n",
    "    if det_labels == ['background']:\n",
    "        # Just return original image\n",
    "        return original_image\n",
    "\n",
    "    # Annotate\n",
    "    annotated_image = original_image\n",
    "    draw = ImageDraw.Draw(annotated_image)\n",
    "    font = ImageFont.truetype(\"./calibril.ttf\", 15)\n",
    "\n",
    "    # Suppress specific classes, if needed\n",
    "    for i in range(det_boxes.size(0)):\n",
    "        if suppress is not None:\n",
    "            if det_labels[i] in suppress:\n",
    "                continue\n",
    "\n",
    "        # Boxes\n",
    "        box_location = det_boxes[i].tolist()\n",
    "        draw.rectangle(xy=box_location, outline=label_color_map[det_labels[i]])\n",
    "        draw.rectangle(xy=[l + 1. for l in box_location], outline=label_color_map[\n",
    "            det_labels[i]])  # a second rectangle at an offset of 1 pixel to increase line thickness\n",
    "        # draw.rectangle(xy=[l + 2. for l in box_location], outline=label_color_map[\n",
    "        #     det_labels[i]])  # a third rectangle at an offset of 1 pixel to increase line thickness\n",
    "        # draw.rectangle(xy=[l + 3. for l in box_location], outline=label_color_map[\n",
    "        #     det_labels[i]])  # a fourth rectangle at an offset of 1 pixel to increase line thickness\n",
    "\n",
    "        # Text\n",
    "        text_size = font.getsize(det_labels[i].upper())\n",
    "        text_location = [box_location[0] + 2., box_location[1] - text_size[1]]\n",
    "        textbox_location = [box_location[0], box_location[1] - text_size[1], box_location[0] + text_size[0] + 4.,\n",
    "                            box_location[1]]\n",
    "        draw.rectangle(xy=textbox_location, fill=label_color_map[det_labels[i]])\n",
    "        draw.text(xy=text_location, text=det_labels[i].upper(), fill='white',\n",
    "                  font=font)\n",
    "    del draw\n",
    "\n",
    "    return annotated_image\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    img_path = 'E:/数据集/PASCAL_VOC/VOCtest2007/VOCtest_06-Nov-2007/VOCdevkit/VOC2007/JPEGImages/009171.jpg'\n",
    "    # img_path = 'E:/复旦/复旦20-21下学期/神经网络课/期中作业/真实图片/8.jpg'\n",
    "    original_image = Image.open(img_path, mode='r')\n",
    "    original_image = original_image.convert('RGB')\n",
    "    detect(original_image, min_score=0.2, max_overlap=0.2, top_k=200).show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
